{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Mack Lab Knowledge Base","text":"<p>Hi everyone!</p> <p>again</p>"},{"location":"arrakis/","title":"Data server: Arrakis","text":""},{"location":"beluga/","title":"Using Beluga HPC system for fmriprep","text":"<p>Beluga is one of the HPC systems under the Digital Research Alliance umbrella. We have access to it without the need for a resource allocation award and it provides 1TB of project space for each PI (i.e., the Mack Lab has 1TB). That's not a ton of space, but it is enough for temporary storage while running fmriprep jobs.</p>"},{"location":"beluga/#logging-into-beluga","title":"Logging into Beluga","text":"<p>Beluga uses the same login credentials as all of the Alliance systems. You can access the login node with SSH via the command line or Visual Studio Code: </p> <pre><code>$ ssh &lt;username&gt;@beluga.computecanada.ca\n</code></pre> <p>Setting up SSH keys through the CCDB is highly recommended so you don't have to worry about entering passwords.</p>"},{"location":"beluga/#initial-setup","title":"Initial setup","text":"<p>We will use apptainer images of fmriprep docker images which means initial setup is minimal. The main requirements are:</p> <ol> <li>fmriprep apptainer images: Docker images of fmriprep versions have to be converted into Apptainer images. If you want to use a new version of fmriprep, follow the instructions from the Alliance and fmriprep. Available images are stored on Beluga in <code>/project/def-mmack/software/containers</code>.</li> <li>TemplateFlow: TemplateFlow is a helper tool that allows programmatically access to standard neuroimaging templates on the fly (i.e., new templates are downloaded from the internet as needed). fmriprep depends on templateflow to work. This presents a problem for HPC systems as most setups, including Beluga, do not allow compute nodes to access the internet. As such, TemplateFlow images must be downloaded separately and made available to fmriprep. This repository is stored on Beluga in <code>/project/def-mmack/software/TemplateFlow</code>.</li> </ol>"},{"location":"beluga/#setup-your-bids-dataset","title":"Setup your BIDS dataset","text":"<p>You will need to copy your BIDS dataset to Beluga. Copy to your lab's project space (e.g., /project/def-mmack/ or /project/def/mschlich/, or see below for an alternative location!). You can use scp commands or sftp apps to do so. You will not want to transfer all of your participants' data at once so as to not fill up the 1TB of project space. But, you need a valid BIDS dataset, so should include code, derivatives, and sub-XXX directories as well as the CHANGES, README, dataset_description.json, participants.json, participants.tsv, and .bidsignore files.  <p>An alternative is to use your scratch space for storing your BIDS dataset. Scratch is a huge but temporary disk space. Any files that are there for longer than 60 days without being modified will be deleted. If you use scratch, you have to be sure to copy results off of Beluga before they are deleted. On Beluga, scratch space is found under the /scratch path (e.g., <code>/scratch/mmack/</code>).</p>"},{"location":"beluga/#add-and-modify-run_fmriprepsh","title":"Add and modify run_fmriprep.sh","text":"<p>The <code>run_fmriprep.sh</code> script included here is an example of how to make the apptainer call to run fmriprep on a single participant. This script should be copied into your BIDS dataset's code directory. </p> <p>The script needs to be updated to reflect your dataset location and any specific parameters you need for fmriprep. At a minimum, you should change the path to your dataset location and confirm which version of fmriprep you want to use which are defined in the script's first two variables:</p> run_fmriprep.sh<pre><code>#/bin/bash\n\n# location of your BIDS dataset (CHANGE THIS!)\nBIDSDIR=/project/def-mmack/projects/funclearn\n\n# fmriprep version \nfpver=23.1.4\n\n# location of fmriprep and templateflow\nSWDIR=/project/def-mmack/software\nTEMPLATEFLOW_HOME=${SWDIR}/templateflow\n# paths for singularity image\nexport APPTAINERENV_FS_LICENSE=/freesurfer_license.txt\nexport APPTAINERENV_TEMPLATEFLOW_HOME=/templateflow\n# create temporary directory\nmkdir -p ${SCRATCH}/tmp\n\n# run fmriprep\napptainer run --cleanenv \\\n  -B ${SWDIR}/freesurfer_license.txt:/freesurfer_license.txt \\\n  -B ${TEMPLATEFLOW_HOME:-$HOME/.cache/templateflow}:/templateflow \\\n  -B ${BIDSDIR}:/data \\\n  -B ${BIDSDIR}/derivatives:/out \\\n  -B ${SCRATCH}/tmp:/work \\\n  ${SWDIR}/containers/fmriprep-${fpver}.sif \\\n  /data /out \\\n  participant \\\n  --participant-label $1 \\\n  --work-dir /work \\\n  --ignore slicetiming \\\n  --fs-no-reconall \\\n  --nthreads 10 \\\n  --notrack \\\n  --output-spaces MNI152NLin2009cAsym T1w\n</code></pre>"},{"location":"beluga/#submit-fmriprep-jobs","title":"Submit fmriprep jobs","text":"<p>To submit a job to Beluga's scheduler, we need a second script: <code>submit_fmriprep.sh</code>: </p> submit_fmriprep.sh<pre><code>#!/bin/bash\n#SBATCH --account=def-mmack (MAYBE CHANGE THIS!)\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=10\n#SBATCH --mem-per-cpu=4G\n#SBATCH --time=2:00:00 (MAYBE CHANGE THIS!)\n#SBATCH --job-name=fmriprep\n#SBATCH --output=/project/def-mmack/mmack/funclearn/code/logs/fmriprep_%j.txt (CHANGE THIS!)\n#SBATCH --mail-user=&lt;your email address&gt; (CHANGE THIS!)\n#SBATCH --mail-type=ALL\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\nmodule load apptainer\n\n# location of your BIDS dataset (CHANGE THIS!)\nPROJDIR=/project/def-mmack/projects/funclearn\n\n${PROJDIR}/code/run_fmriprep.sh $1\n</code></pre> <p>This script needs a bit more modification (look for the CHANGE THIS! comments in the code above), specifically change:</p> <ul> <li><code>--account</code> to your PI's project name (e.g., def-mmack or def-mschlich)</li> <li><code>--time</code> to however much time your fmriprep run needs (see note below)</li> <li><code>--output</code> to a log file path in your BIDS directory</li> <li><code>--mail-user</code> to your email address</li> <li><code>PROJECTDIR</code> to your BIDS dataset path  </li> </ul> <p>What time duration to use?</p> <p>To decide what time duration to use, you want to find a safe minimal duration that isn't too long (otherwise your job will be stuck on the scheduler for longer) but provides enough time to finish the fmriprep job. There will be some variability across participants, so your time duration should have some buffer. Also, depending on what you plan to do with fmriprep can have a huge difference in timing (e.g., including freesurfer in the pipeline can add ~6 hours). Benchmark two participants by setting the time to a higher duration (e.g., 8-10 hours) and see how long the jobs takes to complete. Then take the maximum time and add 0.5-1 hour for your time parameter. As a point of reference, a default fmriprep job without freesurfer for one participant with 1 T1w, 1 fieldmap, 8 BOLD timeseries (~150 volumes each) takes approximately 1 hr 15 mins with 10 cores and 4GB RAM per core (i.e., as defined in the submit_fmriprep.sh script).  </p> <p>With both of scripts updated and copied to your BIDS dataset's code directory, you should be all set to submit a job. To do so, while logged into Beluga, go to your BIDS directory and run the submit_fmriprep.sh script with a specific participant number, e.g.:</p> <pre><code>$ sbatch code/submit_fmriprep.sh 100\n</code></pre> <p>If all goes well, a job will be added to the queue.</p>"},{"location":"beluga/#checking-job-status","title":"Checking job status","text":"<p>There are a few tools for checking the status of running jobs. To get general stats about your running jobs, run the <code>sq</code> command while logged into Beluga. </p> <p>You can also watch a running job by attaching to the node with <code>srun</code> and running htop. First, run <code>sq</code> to get the jobid (i.e., first column of the <code>sq</code> output). Then, run:</p> <pre><code>$ srun --jobid 123456 --pty htop -u $USER\n</code></pre>"},{"location":"beluga/#job-complete-tasks","title":"Job complete tasks","text":"<p>Once your job finishes, copy the fmriprep output in the derivaties folder back to your computer, Arrakis/Trellis, or ix. Finally, remove those files from Beluga, especially if you're using lab project space!</p>"},{"location":"computing/","title":"Overview","text":"<p>The Mack Lab consists of several different computation systems each with their specific purposes:</p> <ul> <li>ix - Our lab's local computation server. It is a multicore system with high RAM and disk space. All our projects that require lots of computation are primarily housed on ix.</li> <li>Arrakis - Our lab's data server. It is a network attached storage server that provides long-term storage and backups.</li> <li>High-performance computing - We leverage the Narval and Niagara HPC systems through the Digital Research Alliance of Canada. These systems are capable of efficiently running many computational tasks in parallel (e.g., MRI preprocessing on many participants at the same time, many iterations of computational model simulations).</li> </ul> <p>Why \"ix\" and \"Arrakis\"?</p>"},{"location":"hpc/","title":"High-performance computing","text":""},{"location":"ix/","title":"Compute server: ix","text":""},{"location":"ix/#cpu-cores","title":"CPU cores","text":"<p>ix has a 20-core processor with hyper threading. This means there are a total of 40 cores available to the system for running processes (hyper threading looks better than it is, a set of hyper threading cores runs about 120% faster than a set of cores without hyper threading). So, that might seem like a lot of cores until eight people are trying to run their analyses all at the same time. In general, it is good to limit the number of cores available to any process. This is accomplished in a lot of different ways. Here's a list (that we should keep adding to):</p> <ul> <li>Environment variables: Setting OMP_NUM_THREADS in the command line ($ export OMP_NUM_THREADS=10) will some times work to limit the number of threads that can be opened by a single process. This requires that the process is multi-processor aware.</li> <li>pymvpa: Multiple cores are critical for searchlight analyses. This requires two things, first setting OMP_NUM_THREADS equal to 1 in the terminal, and second running pymvpa searchlight code that sets nproc to some value in the range 4-10.</li> <li>MRtrix has its own flag for setting the number of threads in all of this functions. Add -nthreads N, where N is 4-10, in any MRtrix command.</li> </ul>"},{"location":"ix/#accessing-data-drives-over-smb","title":"Accessing data drives over SMB","text":"<p>Users can remote connect to ix data drives via SMB such that they are mounted locally through Finder (or god forbid Windows Explorer). To do so, you first need to create a SMB password on ix. Log in via ssh, then run the following command: <pre><code>$ sudo smbpasswd -a &lt;username&gt;\n</code></pre> Type in your smb password and remember it! Then, you need to be added to the sambashare group. Run the <code>groups</code> command from the command line to see the list of groups you belong to. If sambashare is not in the list, ask Mike or a grad student to add you. Finally, on your local computer, create a new remote connection to ix. Here's how to do it in Finder on MacOS: From the Go menu, select Connect to Server... (or hit commmand+k), type in \"smb://ix.mack.psych.utoronto.ca\" in the address bar on top, and click Connect. For the login information, select \"Registered User\" and use your ix username and the SMB password you just set. Check the box to remember the credentials so you don't have to put this in every time you connect. Pick which data drive you want to mount and you'll have direct access to your files on ix. </p>"},{"location":"ix/#managing-projects-on-ix","title":"Managing projects on ix","text":"<p>Projects are stored on ix in the /data and /data2 drives. Each project has a main directory (e.g., <code>/data2/&lt;project name&gt;</code>). Each project is managed by a specific user (or maybe a few users). To be a project manager, your user account will need sudo access. If you don't already have sudo access (or know what this is), ping Mike! </p>"},{"location":"ix/#file-access-policies-on-ix","title":"File access policies on ix","text":"<p>To encourage coding sharing and collaboration but also protect our project files on ix, we have implemented the following policies for file/directory access:</p> <ol> <li>All projects are world readable. This means any user on ix can view and copy files/directories from all projects.</li> <li>Project files/directories can be modified (i.e., create, update, delete operations) by only those users who are members of project groups. Each project will have its own unix group and users of that project will have to be added to the project group to modify the project files.</li> <li>Backups for project files are opt-in. Project managers will have to manually add project directories to the main ix backup list.</li> </ol>"},{"location":"ix/#setting-up-a-new-project","title":"Setting up a new project","text":"<p>New projects on ix require some setup. Follow the instructions below to manage a new project. Note: project management requires a user account with sudo access. If you don't already have sudo access, ping Mike! Also, all of the commands listed below assume you are logged into ix and using a terminal. The <code>$</code> represents a command line prompt</p>"},{"location":"ix/#create-a-project-directory","title":"Create a project directory","text":"<p>Each project will have its own base directory. All files/directories related to this project should be stored within this base directory. This would include subdirectories for pilots as well as the a project's core studies and analyses. Create a new project directory with the following command, where <code>&lt;projectname&gt;</code> is a placeholder for your project's name:</p> <pre><code>$ mkdir /data2/&lt;projectname&gt;\n</code></pre>"},{"location":"ix/#create-a-project-group","title":"Create a project group","text":"<p>Your new project needs a user group! Note, you will need to use the <code>sudo</code> command to create a group. <code>sudo</code> gives you temporary elevated privileges while running a command and requires that you enter your password before the command is completed. To keep things easy, let's use the project's name for the project group. Here's how to create a group:</p> <pre><code>$ sudo groupadd &lt;projectname&gt;\n</code></pre> <p>You can check that the group has been created by running the following command (here we are using funclearn as an example project):</p> <p><pre><code>$ getent group | grep funclearn\nfunclearn:x:1034:\n</code></pre> You should see a line of text that shows the group name and its internal unique group ID (i.e., 1034 in the example). If you see this, your group has been created.</p>"},{"location":"ix/#set-project-directorys-group-and-setgid-bit","title":"Set project directory's group and SetGID bit","text":"<p>The project directory's group needs to be set to the project group:</p> <pre><code>$ sudo chgrp -R &lt;projectname&gt; /data2/&lt;projectname&gt;\n</code></pre> <p>A few notes for the command above: The order of the two arguments for <code>chgrp</code> are first the group name and second the directory path. Also, the <code>-R</code> flag says to additionally change the group for all files/directories contained in the directory. If you just created your project directory and it is empty, the <code>-R</code> flag isn't necessary.</p> <p>To check that your project directory's group has been changed, run this <code>ls</code> command (again, using funclearn as an example):</p> <pre><code>$ ls -ld /data2/funclearn\ndrwxrwxr-x+ 47 mmack funclearn 4096 May 10 11:32 /data2/funclearn\n</code></pre> <p>Quick description of the output: The very first <code>d</code> says that this is a directory. The next string of characters define file permissions for the directory (<code>r</code>=read, <code>w</code>=write, <code>x</code>=execute) for the file owner (first three characters), file group (second three characters), and everyone else (last three characters). If you see the <code>r</code>, <code>w</code>, <code>x</code>, it means that permission for that user or group is set, otherwise (i.e., <code>-</code>) that access is not set. Skip over the number and then the next two columns list the directory owner (<code>mmack</code>) and group (<code>funclearn</code>). If you see your project name in the group column, your project directory's group is all set!</p> <p>Last thing to worry about here is to set the project directory's SetGID bit. \"The what?\", you ask. The SetGID bit on a directory, if set, means that any file or directory created within that directory will have the same group. This is key, we want all of the files/directories within your project directory to have the same project group. To do set the SetGID bit, run this command:</p> <pre><code>$ sudo chmod g+s /data2/&lt;projectname&gt;\n</code></pre> <p>Let's check what happened with that same <code>ls</code> command (using funclearn as an example):</p> <pre><code>$ ls -ld /data2/funclearn\ndrwxrwsr-x+ 47 mmack funclearn 4096 May 10 11:32 /data2/funclearn\n</code></pre> <p>There's one change that should have happened: Look for the <code>s</code> in the group permission characters. The <code>s</code> has replaced the <code>x</code>. This means that SetGID bit has been set and you are in good shape.</p> <p>Existing projects</p> <p>If you are managing a project with data files already on ix (e.g., you are updating an existing project), you will need to change the group for all of the files/directories in the project directory. To do this, run this command (using funclearn as an example):</p> <pre><code>$ sudo chgrp -R funclearn /data2/funclearn\n</code></pre>"},{"location":"ix/#add-users-to-the-project-group","title":"Add users to the project group","text":"<p>Adding users to the project group is necessary for them to be able to modify the project files/directories. To add a user to the project group, run this command (using funclearn as an example):</p> <pre><code>$ sudo usermod -a -G funclearn &lt;username&gt;\n</code></pre> <p>The <code>-a</code> flag says to append the group to the user's existing groups. The <code>-G</code> flag says to add the user to the group. The last argument is the user's username. You can check that the user has been added to the group by running this command (using funclearn as an example):</p> <pre><code>$ groups &lt;username&gt;\n</code></pre> <p>You should see the project group listed in the output. If you don't, try logging out and logging back in. You could also look at the output of the <code>getent</code> command from above to see if the user is listed in the project group:</p> <pre><code>$ getent group | grep funclearn\nfunclearn:x:1034:mmack,&lt;username&gt;\n</code></pre> <p>Don't forget to add yourself to the project group!</p>"},{"location":"ix/#adding-project-files-to-ix-backup","title":"Adding project files to ix backup","text":"<p>ix has a backup system that runs every night. In order to keep things running smoothly, we have to limit the number of files that are backed up. If configured to do so, this system will backup all files/directories from a project except for those that ar easily created with standard pipelines. As policy, we will not backup the following files/directories:</p> <ul> <li>Raw nifti files (e.g., sub-XXX in the root BIDS directory). These files are easily recreated from the raw DICOM files which are backed up to Arrakis or Trellis. </li> <li>fmriprep output. These files are easily recreated from the raw nifti files and the specific version of fmriprep you are using for your project.</li> <li>Freesurfer output. These files are easily recreated from the raw nifti files and the specific version of freesurfer you are using (likely bundled with fmriprep).</li> <li>Any temporary files/directories (e.g., tmp_dcm2bids)</li> </ul> <p>By default, we will include everyting else in the project directory in the backup. </p> <p>To add your project to the backup list:</p> <ol> <li>Ping Mike with your project name and root directory (e.g., /data2/funclearn)</li> <li>Create a file named <code>backup_exclude.txt</code> in the root directory of your project. This file should contain a list of files/directories that you want to exclude from the backup. Each file/directory should be on its own line. Here's an example from funclearn which includes all the noted exclusions from above: <pre><code>/data2/funclearn/sub-*/\n/data2/funclearn/sourcedata/\n/data2/funclearn/tmp_dcm2bids/\n/data2/funclearn/derivatives/fmriprep/\n/data2/funclearn/derivatives/freesurfer/\n</code></pre> The file/directory paths must be absolute paths. Also, the <code>*</code> is a wildcard character that matches any string. So, <code>sub-*</code> will match any file/directory that starts with <code>sub-</code> (i.e., the raw nifti files for each participant).</li> </ol> <p>Important note: If your project directory is a BIDS root, you will need to add <code>backup_exclude.txt</code> to your <code>.bidsignore</code> file to pass BIDS validation checks.</p>"},{"location":"ix/#docker-on-ix","title":"Docker on ix","text":"<p>To ensure that our file permissions setup works with Docker, we need to specify in docker calls which user and group ID to use while that call is completed. Setting these options means that any files and directories that are created by the docker call will have the permissions of the user and group ID specified. If this isn't set, all files/directories are owned by root. Here's how to set the user and group IDs.</p> <p>First, you need your user ID and the project group ID for the data you are working on. These IDs are numbers associated with user and group accounts. An easy way to retrieve this is to run the <code>id</code> command, which lists both your user id (i.e., uid) and all the IDs for the groups you belong to: <pre><code>$ id\nuid=1000(mmack) gid=1000(mmack) groups=1000(mmack),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),110(lxd),120(sambashare),1001(macklab),1011(bearholiday),1012(funclearn),1014(bighipp),1015(birthday),1016(clusterbake),1019(difflearn),1020(dtilearn),1021(eyelearn),1024(gardener),1027(videoschema),1030(em-hddm),1031(hippcircuit),1032(docker),1033(scenewheel),1034(aa_diffodd)\n</code></pre> Second, you should add a <code>--user</code> argument to your docker calls. For example, if I wanted to run fmriprep via docker on the funclearn data, my docker call would look like this: <pre><code>$ docker run -it --rm --user 1000:1012 \\\n    -v /data/software/fs_license.txt:/opt/freesurfer/license.txt \\\n    -v /data2/templateflow:/templateflow \\\n    -v /data2/funclearn:/data:ro \\\n    -v /data2/funclearn/derivatives/fmriprep:/out \\\n    nipreps/fmriprep:23.1.4 \\\n    /data /out participant \\\n    --ignore slicetiming \\\n    --participant_label 100 \\\n    --nthreads 10 \\\n    --notrack \\\n    --skip-bids-validation \\\n    --output-spaces MNI152NLin2009cAsym func\n</code></pre> Note the <code>--user 1000:1012</code> argument in the first line. This sets the user to be me (uid=1000) and group to be funclearn (group ID = 1012) with these numbers pulled from the <code>id</code> command above. By settingi the <code>--user</code> argument, all files and directories created by the fmriprep call will have me as the owner and funclearn as the group. </p> <p>NOTE: If you are using a newer version of fmriprep (ver 22.0 and newer), you must explictly define the name of the output directory inside the derivatives directory (e.g., <code>-v /data2/funclearn/derivatives/fmriprep:/out</code>). Prior to this, fmriprep defaulted to creating a directory named fmriprep. To make this all work well with our permissions setup, please create your fmriprep derivatives folder first before running fmriprep on your participants. If you don't, the output folder will be created by docker with root as owner and group. If this happens, all is not lost. You can change the group to your project group (e.g., <code>$ sudo chgrp -R funclearn /data2/funclearn/derivatives/fmriprep</code>), then subsequent fmriprep calls will work without write permission errors.</p>"},{"location":"mri_analysis/","title":"Overview","text":"<p>Included here are resources and tips for best practices within the lab. There are many general-purpose resources available online for MRI analysis, check the links below!</p>"},{"location":"mri_analysis/#lab-specific-resources","title":"Lab-specific resources","text":"<ul> <li>MRI quality control</li> <li>Automatic segmentation of hippocampal subfields (ASHS)</li> <li>fMRIPrep on ix and Beluga</li> </ul>"},{"location":"mri_analysis/#external-resources","title":"External resources","text":"<ul> <li>Andy's Brain Book</li> <li>Princeton Handbook for Reproducible Neuroimaging</li> <li>Dart Brains Tutorial Course</li> </ul>"},{"location":"mri_ashs/","title":"ASHS","text":"<p>ASHS is a fast method for automatically segmenting hippocampal subfields on a high-resolution T2-weighted volume. ASHS can be run locally on ix or through a cloud-based distributed segmentation service (DSS).</p>"},{"location":"mri_ashs/#ashs-on-ix","title":"ASHS on ix","text":"<p>Running ASHS on ix is simple. First, navigate to your project's BIDS directory. Then, activate the <code>mri-preproc</code> conda environment:</p> <pre><code>$ conda activate mri-preproc\n</code></pre> <p>Next, set an environment variable to the path of the ASHS binaries:</p> <pre><code>$ export ASHS_ROOT=/data/software/ashs/ashs\n</code></pre> <p>Finally, you can run ASHS on your participant. To do so, you will need the path to their T1 and T2 volumes and the path to the Princeton Young Adult atlas. Here, I'll use an example participant from the gardener project:</p> <pre><code>$ ${ASHS_ROOT}/bin/ashs_main.sh \\\n    -I sub-028 \\\n    -a /data/software/ashs/atlases/ashs_atlas_princeton \\\n    -g /data2/gardener/sub-028/anat/sub-028_T1w_crop.nii.gz \\\n    -f /data2/gardener/sub-028/anat/sub-028_T2w.nii.gz \\\n    -w /data2/gardener/derivatives/ashs/sub-028\n</code></pre> <p>In the above command, <code>-I</code> is the subject code, <code>-a</code> sets the path to the atlas (this is fixed, you won't change this!), <code>-g</code> and <code>-f</code> are the paths to the participant's T1 and T2 volumes, and <code>-w</code> is the output directory. </p> <p>ASHS takes 10-15 minutes to finish a segmentation. The output directory will contain all the files ASHS generates including measures of ICV and subfield volumes. The final segmentations and measures are in the <code>final</code> directory, look for the _left_lfseg_corr_usegray.nii.gz and _right_lfseg_corr_usegray.nii.gz volumes. The two hemispheres are segmented separately and will be in the same space as the T2 volume. They can be quickly combined with the following fslmaths command:</p> <pre><code>$ fslmaths sub-028_left_lfseg_corr_usegray.nii.gz -add sub-028_right_lfseg_corr_usegray.nii.gz sub-028_ashs.nii.gz\n</code></pre>"},{"location":"mri_ashs/#ashs-via-dss","title":"ASHS via DSS","text":"<p>The ASHS documentation provides a helpful tutorial for how to use their cloud-based service. Note that we typically use the Princeton Young Adult atlas which is different from the atlas noted in the documentation. A few notes about DSS: It is free to use, works well, and can be run interactively with itk-SNAP which makes checking the quality of the segmentation straightforward. The downsides are that a limited number of segmentations can be performed at the same time and submitting jobs through itk-SNAP can be tedious with a larger number of participants.</p>"},{"location":"mri_qc/","title":"Best practices for MRI quality control","text":""},{"location":"narval/","title":"Using Narval HPC system","text":"<p>Narval is one of the HPC systems under the Digital Research Alliance umbrella. We have access to it without the need for a resource allocation award and we currently have 20TB of project space and 40TB of archive space.</p>"},{"location":"narval/#logging-into-narval","title":"Logging into Narval","text":"<p>Narval uses the same login credentials as all of the Alliance systems. You can access the login node with SSH via the command line or Visual Studio Code: </p> <pre><code>$ ssh &lt;username&gt;@narval.alliancecan.ca\n</code></pre> <p>Setting up SSH keys through the CCDB is highly recommended so you don't have to worry about entering passwords.</p>"},{"location":"narval/#initial-setup","title":"Initial setup","text":"<p>When you first log into Narval, you will need to set up your environment. This includes setting up your bash profile to enable group read/write access and a few other things:</p> <ol> <li>Allow group read/write permission by changing umask in your bash profile: Umask defines the default permissions for new files and directories that you create. We want to set the umask to 007 so that new files and directories are created with group read and write permissions. This is important for sharing data with other lab members. To do this, add the following line to your <code>~/.bash_profile</code> file:</li> </ol> <pre><code>umask 007\n</code></pre> <ol> <li>Set environment variables for software directories: All shared software and data directories are located in <code>/project/def-mmack/</code>. You can set environment variables in your <code>~/.bash_profile</code> file to make it easier to access these directories at the command line and in scripts. Add the following lines to your <code>~/.bash_profile</code> file:</li> </ol> <pre><code>export SWDIR=/project/def-mmack/software\nexport CONTAINERS=${SWDIR}/containers\nexport TEMPLATEFLOW_HOME=${SWDIR}/templateflow\n</code></pre> <p>A quick rundown of these directories: * <code>SWDIR</code>: This is the main software directory where all shared software is stored. You can access this directory with the <code>$SWDIR</code> variable. * <code>CONTAINERS</code>: This is where all the apptainer images are stored. You can access this directory with the <code>$CONTAINERS</code> variable. * <code>TEMPLATEFLOW_HOME</code>: fmriprep uses TemplateFlow for managing neuroimaging atlases and templates (see below for more details). We keep an offline copy of TemplateFlow in $SWDIR. You can access this directory with the <code>$TEMPLATEFLOW_HOME</code> variable.</p> <p>After making these changes, log out and log back for them to be applied.</p>"},{"location":"narval/#project-organization","title":"Project organization","text":"<p>All projects should be stored in the lab's project space: <code>/project/def-mmack/projects/</code>. Use the same project name as you would on ix. For example, if your project is called <code>funclearn</code>, you would create a directory <code>/project/def-mmack/projects/funclearn/</code> to store all your data, code, and results. This <code>projects</code> directory is shared with all lab members (i.e., the lab group has read and write access), so you can easily share data and code with others. Also, the setguid bit is set on the <code>projects</code> directory, so all files and directories created within it will inherit the group ownership of the <code>projects</code> directory.</p> <p>There are also user-specific directories in the project space (e.g., <code>/project/def-mmack/mmack</code>). These directories are by default set to be accessible only to the user. It is possible to change permissions to allow group access, but that would be something you have to manage yourself. Feel free to use this user directory in the project space, but keep in mind that all data, code, and analyses should be accessible to the group for easy access. </p>"},{"location":"narval/#example-running-fmriprep-on-narval","title":"Example: Running fmriprep on Narval","text":"<p>A typical job we will run on Narval is using fmriprep for data preprocessing. Below is a quick tutorial for doing this. Running fmriprep requires: </p> <ol> <li>fmriprep apptainer images: Docker images of fmriprep versions have to be converted into Apptainer images. If you want to use a new version of fmriprep, follow the instructions from the Alliance and fmriprep. Available images are stored on Narval in <code>/project/def-mmack/software/containers</code>.</li> <li>TemplateFlow: TemplateFlow is a helper tool that allows programmatically access to standard neuroimaging templates on the fly (i.e., new templates are downloaded from the internet as needed). fmriprep depends on templateflow to work. This presents a problem for HPC systems as most setups, including Narval, do not allow compute nodes to access the internet. As such, TemplateFlow images must be downloaded separately and made available to fmriprep. This repository is stored on Narval in <code>/project/def-mmack/software/TemplateFlow</code>.</li> </ol>"},{"location":"narval/#setup-your-bids-dataset","title":"Setup your BIDS dataset","text":"<p>You will need to copy your BIDS dataset to Narval. Copy to your lab's project space (i.e., <code>/project/def-mmack/projects/&lt;project name&gt;</code>). You can use scp commands or sftp apps to do so. You need a valid BIDS dataset, so should include code, derivatives, and sub-XXX directories as well as the CHANGES, README, dataset_description.json, participants.json, participants.tsv, and .bidsignore files. </p> <p>An alternative is to use your scratch space for storing your BIDS dataset. Scratch is a huge but temporary disk space. Any files that are there for longer than 60 days without being modified will be deleted. If you use scratch, you have to be sure to copy results off of Narval before they are deleted. On Narval, scratch space is found under the /scratch path (e.g., <code>/scratch/mmack/</code>).</p>"},{"location":"narval/#add-and-modify-run_fmriprepsh","title":"Add and modify run_fmriprep.sh","text":"<p>The <code>run_fmriprep.sh</code> script included here is an example of how to make the apptainer call to run fmriprep on a single participant. This script should be copied into your BIDS dataset's code directory. </p> <p>The script needs to be updated to reflect your dataset location and any specific parameters you need for fmriprep. At a minimum, you should change the path to your dataset location and confirm which version of fmriprep you want to use which are defined in the script's first two variables:</p> run_fmriprep.sh<pre><code>#/bin/bash\n\n# location of your BIDS dataset (CHANGE THIS!)\nBIDSDIR=/project/def-mmack/projects/funclearn\n\n# fmriprep version \nfpver=23.1.4\n\n# location of fmriprep and templateflow\nSWDIR=/project/def-mmack/software\nTEMPLATEFLOW_HOME=${SWDIR}/templateflow\n# paths for singularity image\nexport APPTAINERENV_FS_LICENSE=/freesurfer_license.txt\nexport APPTAINERENV_TEMPLATEFLOW_HOME=/templateflow\n# create temporary directory\nmkdir -p ${SCRATCH}/tmp\n\n# run fmriprep\napptainer run --cleanenv \\\n  -B ${SWDIR}/freesurfer_license.txt:/freesurfer_license.txt \\\n  -B ${TEMPLATEFLOW_HOME:-$HOME/.cache/templateflow}:/templateflow \\\n  -B ${BIDSDIR}:/data \\\n  -B ${BIDSDIR}/derivatives:/out \\\n  -B ${SCRATCH}/tmp:/work \\\n  ${SWDIR}/containers/fmriprep-${fpver}.sif \\\n  /data /out \\\n  participant \\\n  --participant-label $1 \\\n  --work-dir /work \\\n  --ignore slicetiming \\\n  --fs-no-reconall \\\n  --nthreads 10 \\\n  --notrack \\\n  --output-spaces MNI152NLin2009cAsym T1w\n</code></pre>"},{"location":"narval/#submit-fmriprep-jobs","title":"Submit fmriprep jobs","text":"<p>To submit a job to Narval's scheduler, we need a second script: <code>submit_fmriprep.sh</code>: </p> submit_fmriprep.sh<pre><code>#!/bin/bash\n#SBATCH --account=def-mmack (MAYBE CHANGE THIS!)\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=10\n#SBATCH --mem-per-cpu=4G\n#SBATCH --time=2:00:00 (MAYBE CHANGE THIS!)\n#SBATCH --job-name=fmriprep\n#SBATCH --output=/project/def-mmack/projects/funclearn/code/logs/fmriprep_%j.txt (CHANGE THIS!)\n#SBATCH --mail-user=&lt;your email address&gt; (CHANGE THIS!)\n#SBATCH --mail-type=ALL\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\nmodule load apptainer\n\n# location of your BIDS dataset (CHANGE THIS!)\nPROJDIR=/project/def-mmack/projects/funclearn\n\n${PROJDIR}/code/run_fmriprep.sh $1\n</code></pre> <p>This script needs a bit more modification (look for the CHANGE THIS! comments in the code above), specifically change:</p> <ul> <li><code>--account</code> to your PI's project name (e.g., def-mmack or def-mschlich)</li> <li><code>--time</code> to however much time your fmriprep run needs (see note below)</li> <li><code>--output</code> to a log file path in your BIDS directory</li> <li><code>--mail-user</code> to your email address</li> <li><code>PROJECTDIR</code> to your BIDS dataset path  </li> </ul> <p>What time duration to use?</p> <p>To decide what time duration to use, you want to find a safe minimal duration that isn't too long (otherwise your job will be stuck on the scheduler for longer) but provides enough time to finish the fmriprep job. There will be some variability across participants, so your time duration should have some buffer. Also, depending on what you plan to do with fmriprep can have a huge difference in timing (e.g., including freesurfer in the pipeline can add ~6 hours). Benchmark two participants by setting the time to a higher duration (e.g., 8-10 hours) and see how long the jobs takes to complete. Then take the maximum time and add 0.5-1 hour for your time parameter. As a point of reference, a default fmriprep job without freesurfer for one participant with 1 T1w, 1 fieldmap, 8 BOLD timeseries (~150 volumes each) takes approximately 1 hr 15 mins with 10 cores and 4GB RAM per core (i.e., as defined in the submit_fmriprep.sh script).  </p> <p>With both of scripts updated and copied to your BIDS dataset's code directory, you should be all set to submit a job. To do so, while logged into Narval, go to your BIDS directory and run the submit_fmriprep.sh script with a specific participant number, e.g.:</p> <pre><code>$ sbatch code/submit_fmriprep.sh 100\n</code></pre> <p>If all goes well, a job will be added to the queue.</p>"},{"location":"narval/#checking-job-status","title":"Checking job status","text":"<p>There are a few tools for checking the status of running jobs. To get general stats about your running jobs, run the <code>sq</code> command while logged into Narval. </p> <p>You can also watch a running job by attaching to the node with <code>srun</code> and running htop. First, run <code>sq</code> to get the jobid (i.e., first column of the <code>sq</code> output). Then, run:</p> <pre><code>$ srun --jobid 123456 --pty htop -u $USER\n</code></pre>"},{"location":"narval/#job-complete-tasks","title":"Job complete tasks","text":"<p>Once your job finishes, copy the fmriprep output in the derivaties folder back to your computer, Arrakis/Trellis, or ix. </p>"}]}